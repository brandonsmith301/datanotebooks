{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Imports and Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.linear_model import LogisticRegression\nimport optuna\nfrom optuna.samplers import TPESampler\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-04T09:46:46.033980Z","iopub.execute_input":"2023-02-04T09:46:46.034798Z","iopub.status.idle":"2023-02-04T09:46:46.042470Z","shell.execute_reply.started":"2023-02-04T09:46:46.034752Z","shell.execute_reply":"2023-02-04T09:46:46.041316Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"markdown","source":"#### Data pre-processing and functions","metadata":{}},{"cell_type":"code","source":"class WineDataProcessor:\n    \n    def __init__(self, train_df, test_df):\n        self.train_df = train_df\n        self.test_df = test_df\n\n    def add_columns(self, df):\n        df['total_acid'] = df['fixed acidity'] + df['volatile acidity'] + df['citric acid']\n        df['acid/density'] = df['total_acid']  / df['density']\n        df['alcohol_density'] = df['alcohol']  * df['density']\n        df['sulphate/density'] = df['sulphates']  / df['density']\n        df['sulphates/acid'] = df['sulphates'] / df['volatile acidity']\n        df['sulphates/chlorides'] = df['sulphates'] / df['chlorides']\n        df['sulphates*alcohol'] = df['sulphates'] / df['alcohol']\n        return df\n    \n    def process_data(self):\n        self.train_df = self.add_columns(self.train_df).assign(\n            pH_round1 = self.train_df['pH'].round(1),\n            log1p_residual_sugar = np.log1p(self.train_df['residual sugar']),\n            citric_acid_per_alcohol = np.where(self.train_df['alcohol'] == 0, 0, self.train_df['citric acid'] / self.train_df['alcohol']),\n        ).assign(\n            citric_acid_type = lambda df: df['citric acid'].apply(lambda x: 0 if x == 0 else (1 if x == 0.49 else 2)),\n            alcohol_mean_groupby_pH = lambda df: df.groupby('pH_round1')['alcohol'].transform('mean')\n        )\n\n        self.test_df = self.add_columns(self.test_df).assign(\n            pH_round1 = self.test_df['pH'].round(1),\n            log1p_residual_sugar = np.log1p(self.test_df['residual sugar']),\n            citric_acid_per_alcohol = np.where(self.test_df['alcohol'] == 0, 0, self.test_df['citric acid'] / self.test_df['alcohol']),\n        ).assign(\n            citric_acid_type = lambda df: df['citric acid'].apply(lambda x: 0 if x == 0 else (1 if x == 0.49 else 2)),\n            alcohol_mean_groupby_pH = lambda df: df.groupby('pH_round1')['alcohol'].transform('mean')\n        )\n\n        return self.train_df, self.test_df\n    \nclass OptunaRounder: # credit to : kotrying\n\n    def __init__(self, y_true, y_pred):\n        self.y_true = y_true\n        self.y_pred = y_pred\n        self.labels = np.unique(y_true)\n\n    def __call__(self, trial):\n        thresholds = []\n        for i in range(len(self.labels) - 1):\n            low = max(thresholds) if i > 0 else min(self.labels)\n            high = max(self.labels)\n            t = trial.suggest_float(f't{i}', low, high)\n            thresholds.append(t)\n        try:\n            opt_y_pred = self.adjust(self.y_pred, thresholds)\n        except: return 0\n        return cohen_kappa_score(self.y_true, opt_y_pred, weights='quadratic')\n\n    def adjust(self, y_pred, thresholds):\n        opt_y_pred = pd.cut(y_pred,\n                            [-np.inf] + thresholds + [np.inf],\n                            labels=self.labels)\n        return opt_y_pred\n\ndef read_csv(path):\n    \"\"\"\n    Reads a CSV file from the given path and returns the Pandas dataframe.\n    \"\"\"\n    return pd.read_csv(path)\n\ndef train_lgbm_model(X_train, y_train, X_val, y_val, lgb_params_best):\n    \"\"\"\n    Trains a LightGBM model with the given training and validation data.\n    Returns the validation predictions, score and trained model.\n    \"\"\"\n    lgbm_model = lgb.LGBMClassifier(**lgb_params_best)\n    lgbm_model.fit(X_train, y_train, eval_set= [(X_val,y_val)], early_stopping_rounds=200, verbose=500)\n    lgbm_pred_val = lgbm_model.predict(X_val)\n    lgbm_score = cohen_kappa_score(y_val, lgbm_pred_val, weights='quadratic')\n    return lgbm_pred_val, lgbm_score, lgbm_model\n\ndef train_xgboost_model(X_train, y_train, X_val, y_val, xgb_params_best):\n    \"\"\"\n    Trains a XGBoost model with the given training and validation data.\n    Returns the validation predictions, score and trained model.\n    \"\"\"\n    xgb_model = xgb.XGBClassifier(**xgb_params_best)\n    xgb_model.fit(X_train, y_train, eval_set= [(X_val,y_val)], early_stopping_rounds=200, verbose=500)\n    xgb_pred_val = xgb_model.predict(X_val)\n    xgb_score = cohen_kappa_score(y_val, xgb_pred_val, weights='quadratic')\n    return xgb_pred_val, xgb_score, xgb_model\n\ndef train_logistic_regression_model(X_train, y_train, X_val, y_val, logreg_params):\n    \"\"\"\n    Trains a Logistic Regression model with the given training and validation data.\n    Returns the validation predictions, score and trained model.\n    \"\"\"\n    logreg_model = LogisticRegression(**logreg_params)\n    logreg_model.fit(X_train, y_train)\n    logreg_pred_val = logreg_model.predict(X_val)\n    logreg_score = cohen_kappa_score(y_val, logreg_pred_val, weights='quadratic')\n    return logreg_pred_val, logreg_score, logreg_model\n\ndef predict_with_threshold(model, test_df, score, threshold):\n    \"\"\"\n    Predicts the test data using the given model.\n    Returns the predictions and score only if the score is greater than the given threshold, else None.\n    \"\"\"\n    if score > threshold:\n        test_preds = model.predict(test_df)\n        return test_preds, score\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-02-04T09:46:46.044299Z","iopub.execute_input":"2023-02-04T09:46:46.044899Z","iopub.status.idle":"2023-02-04T09:46:46.073385Z","shell.execute_reply.started":"2023-02-04T09:46:46.044856Z","shell.execute_reply":"2023-02-04T09:46:46.071925Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/playground-series-s3e5/train.csv\"\nTEST_PATH = \"/kaggle/input/playground-series-s3e5/test.csv\"\nADDITIONAL_PATH = \"/kaggle/input/wine-quality-dataset/WineQT.csv\"\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\nadditional = pd.read_csv(ADDITIONAL_PATH)\n\n# Concatenate train_df and additional and reset the index\ntrain_df = pd.concat([train_df, additional], ignore_index=True)\n\n# Drop the 'Id' column from both dataframes\ntrain_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)\n\n# Pre-processing\nprocessor = WineDataProcessor(train_df, test_df)\ntrain_df, test_df = processor.process_data()\n\n# Define features and target\nfeatures = ['sulphate/density', 'alcohol_density', 'alcohol', 'sulphates'] # significantly improved my model choosing just these features (credit to : OSCAR AGUILAR)\ntarget = 'quality'\n\n# Change the quality column to 0, 1, 2, 3, 4, 5\ntrain_df['quality'] = train_df['quality'] - 3","metadata":{"execution":{"iopub.status.busy":"2023-02-04T09:46:46.075766Z","iopub.execute_input":"2023-02-04T09:46:46.076306Z","iopub.status.idle":"2023-02-04T09:46:46.132599Z","shell.execute_reply.started":"2023-02-04T09:46:46.076265Z","shell.execute_reply":"2023-02-04T09:46:46.131389Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"#### Hypertune with Optuna","metadata":{}},{"cell_type":"code","source":"def objective(trial, model_type):\n    # calculate the number of unique target classes\n    num_classes = len(train_df[target].unique())\n\n    # scale positive weight is used to adjust the balance between positive and negative samples\n    scale_pos_weight = [0] * num_classes\n    for count, value in enumerate(train_df[target].unique()):\n        num_neg_samples = len(train_df[train_df[target] != value])\n        num_pos_samples = len(train_df[train_df[target] == value])\n        scale_pos_weight[count] = num_neg_samples / num_pos_samples if num_pos_samples > 0 else 1\n    scale_pos_weight = float(np.mean(scale_pos_weight))\n\n    # initialize the fold scores list\n    fold_scores = []\n\n    # choose the number of folds for cross-validation (cv)\n    n_cv = trial.suggest_int('n_cv', 3, 10)\n    # use StratifiedKFold for cross-validation\n    cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=42)\n    \n    # choose the model type and train accordingly\n    if model_type == \"lgbm\":\n        \n        lgb_params_optuna = {\n            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 2, 10),\n            'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),\n            'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 1.0),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 1.0),\n            'objective': 'multiclass',\n            'random_state': 42,\n            'scale_pos_weight': scale_pos_weight\n        }\n        \n        for i, (train_idx, val_idx) in enumerate(cv.split(train_df[features], train_df[target])):\n            X_train, y_train = train_df.loc[train_idx, features], train_df.loc[train_idx, target]\n            X_val, y_val = train_df.loc[val_idx, features], train_df.loc[val_idx, target]\n\n            _, lgbm_score, lgbm_model = train_lgbm_model(X_train, y_train, X_val, y_val, lgb_params_optuna)\n            # add the score of each fold to the fold_scores list\n            fold_scores.append(lgbm_score)\n            \n    elif model_type == 'xgb':\n        \n        xgb_params = {\n            'max_depth': trial.suggest_int('max_depth', 2, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n            'gamma': trial.suggest_float('gamma', 0, 1),\n            'subsample': trial.suggest_float('subsample', 0.5, 1),\n            'objective': 'multi:softmax',\n            'num_class': num_classes,\n            'random_state': 42,\n            'tree_method': trial.suggest_categorical('tree_method', ['auto', 'exact', 'approx', 'hist'])\n        }\n        \n        for i, (train_idx, val_idx) in enumerate(cv.split(train_df[features], train_df[target])):\n            X_train, y_train = train_df.loc[train_idx, features], train_df.loc[train_idx, target]\n            X_val, y_val = train_df.loc[val_idx, features], train_df.loc[val_idx, target]\n\n            _, xgb_score, xgb_model = train_xgboost_model(X_train, y_train, X_val, y_val, model_params)\n            # add the score of each fold to the fold_scores list\n            fold_scores.append(xgb_score)\n        \n    else:\n        \n        lr_params = {\n            'C': trial.suggest_float('C', 0.001, 100),\n            'solver': trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag']),\n            'max_iter': trial.suggest_int('max_iter', 100, 10000),\n            'multi_class': 'auto',\n            'random_state': 42\n        }\n        \n        for i, (train_idx, val_idx) in enumerate(cv.split(train_df[features], train_df[target])):\n            X_train, y_train = train_df.loc[train_idx, features], train_df.loc[train_idx, target]\n            X_val, y_val = train_df.loc[val_idx, features], train_df.loc[val_idx, target]\n\n            _,lr_score, lr_model = train_logistic_regression_model(X_train, y_train, X_val, y_val, model_params)\n            # add the score of each fold to the fold_scores list\n            fold_scores.append(lr_score)\n            \n    return np.mean(fold_scores)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T09:46:46.309548Z","iopub.execute_input":"2023-02-04T09:46:46.310962Z","iopub.status.idle":"2023-02-04T09:46:46.333989Z","shell.execute_reply.started":"2023-02-04T09:46:46.310910Z","shell.execute_reply":"2023-02-04T09:46:46.332928Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":"#### Best params from Optuna","metadata":{}},{"cell_type":"code","source":"# Best params\nlgb_params_best = {\n    \"boosting_type\": \"goss\",\n    \"num_leaves\": 38,\n    \"learning_rate\": 0.2845475116961482,\n    \"n_estimators\": 290,\n    \"max_depth\": 4,\n    \"min_child_samples\": 72,\n    \"subsample\": 0.8166188575050211,\n    \"colsample_bytree\": 0.27881478520246883,\n    \"reg_alpha\": 0.411886427222449,\n    \"reg_lambda\": 0.9994503751899053\n    }\n\nxgb_params_best = {\n    'max_depth': 6,\n    'learning_rate': 0.25370485441912793,\n    'n_estimators': 990,\n    'colsample_bytree': 0.6838592328324482,\n    'gamma': 0.4715095153106331,\n    'subsample': 0.9167549097930624,\n    'tree_method': 'auto'\n    }\n\nlr_params_best = {\n    'C': 20.949884622412462,\n    'solver': 'newton-cg',\n    'max_iter': 6591\n    }","metadata":{"execution":{"iopub.status.busy":"2023-02-04T09:46:46.336059Z","iopub.execute_input":"2023-02-04T09:46:46.336445Z","iopub.status.idle":"2023-02-04T09:46:46.349296Z","shell.execute_reply.started":"2023-02-04T09:46:46.336402Z","shell.execute_reply":"2023-02-04T09:46:46.347916Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"markdown","source":"#### K-fold and model","metadata":{}},{"cell_type":"code","source":"# Set the number of folds\nk = 10\n\n# Initialize StratifiedKFold\ncv = StratifiedKFold(k, shuffle=True, random_state=42)\n\n# Initialize lists for fold scores, test predictions, OOF predictions and true OOF values\nfold_scores = []\ntest_preds = []\noof_preds = []\noof_true = []\n\n# Loop through each fold\nfor i, (train_idx, val_idx) in enumerate(cv.split(train_df[features], train_df[target])):\n    # Get the training and validation sets\n    X_train, y_train = train_df.loc[train_idx, features], train_df.loc[train_idx, target]\n    X_val, y_val = train_df.loc[val_idx, features], train_df.loc[val_idx, target]\n        \n    # XGB classifier\n    model1 = xgb.XGBClassifier(**xgb_params_best)\n    model1.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n              early_stopping_rounds=200, verbose=200)\n\n    # Get the predictions on the validation set\n    pred_val1 = model1.predict(X_val)\n\n    # Calculate the cohen kappa score\n    XGB_score = cohen_kappa_score(y_val, pred_val1, weights='quadratic')\n\n    if XGB_score > 0.36:\n        test_preds.append(model1.predict(test_df[features]))\n        fold_scores.append(XGB_score)\n\n    # LGB classifier    \n    model2 = lgb.LGBMClassifier(**lgb_params_best)\n    model2.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n              early_stopping_rounds=200, verbose=False)\n\n    # Get the predictions on the validation set\n    pred_val2 = model2.predict(X_val)\n\n    # Calculate the cohen kappa score\n    LGB_score = cohen_kappa_score(y_val, pred_val2, weights='quadratic')\n\n    if LGB_score > 0.36:\n        test_preds.append(model2.predict(test_df[features]))\n        fold_scores.append(LGB_score)\n\n    # LG regression  \n    model3 = LogisticRegression(**lr_params_best)\n    model3.fit(X_train, y_train)\n\n    # Get the predictions on the validation set\n    pred_val3 = model3.predict(X_val)\n\n    # Calculate the cohen kappa score\n    LG_score = cohen_kappa_score(y_val, pred_val3, weights='quadratic')\n\n    if LG_score > 0.36:\n        test_preds.append(model3.predict(test_df[features]))\n        fold_scores.append(LG_score)\n                \n    # Extend the true OOF values\n    oof_preds.extend(np.mean([pred_val1,pred_val2,pred_val3],axis=0))\n    oof_true.extend(y_val)\n    \nobjective = OptunaRounder(oof_true, oof_preds)\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, timeout=100)\n\n# Best thresholds & score\nbest_thresholds = sorted(study.best_params.values())\noof_pred_opt = objective.adjust(oof_preds, best_thresholds)\nbest_score = cohen_kappa_score(oof_true, oof_pred_opt, weights='quadratic')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-04T10:13:17.421150Z","iopub.execute_input":"2023-02-04T10:13:17.421874Z","iopub.status.idle":"2023-02-04T10:13:17.428765Z","shell.execute_reply.started":"2023-02-04T10:13:17.421809Z","shell.execute_reply":"2023-02-04T10:13:17.427368Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"# Submission\nsub = pd.read_csv('/kaggle/input/playground-series-s3e5/sample_submission.csv')\ntest_preds = np.array(test_preds).mean(axis=0) \nopt_test_preds = objective.adjust(test_preds, best_thresholds).astype(int) + 3\nsub[target] = opt_test_preds\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:08:17.585657Z","iopub.execute_input":"2023-02-04T10:08:17.586064Z","iopub.status.idle":"2023-02-04T10:08:17.601325Z","shell.execute_reply.started":"2023-02-04T10:08:17.586032Z","shell.execute_reply":"2023-02-04T10:08:17.600033Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"results = sub['quality'].value_counts()\nresults.plot.bar(color=['#1B262C', '#0F4C75', '#3282B8', '#BBE1FA'], rot=0)\nplt.title(\"Quality\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:11:43.369260Z","iopub.execute_input":"2023-02-04T10:11:43.369972Z","iopub.status.idle":"2023-02-04T10:11:43.559710Z","shell.execute_reply.started":"2023-02-04T10:11:43.369930Z","shell.execute_reply":"2023-02-04T10:11:43.558768Z"},"trusted":true},"execution_count":182,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}